{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training v3 - MLP com Otimização de Hiperparâmetros\n",
        "\n",
        "## Objetivo\n",
        "Treinar modelo MLP com busca de hiperparâmetros usando:\n",
        "1. Análise Exploratória de `1_analise_exploratoria.ipynb`\n",
        "2. Pré-processamento de `2_preprocess.ipynb`\n",
        "3. Modelo Avançado de `v3_model.ipynb`\n",
        "\n",
        "## Diferenças v3\n",
        "- Busca de hiperparâmetros com múltiplos seeds\n",
        "- Safe MSE com clipping para evitar overflow\n",
        "- Validação temporal com TimeSeriesSplit\n",
        "- Seleção de melhor combinação por métricas\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TRAINING V3 - MLP COM OTIMIZAÇÃO DE HIPERPARÂMETROS\n",
            "======================================================================\n",
            "\n",
            "Notebooks Reutilizados:\n",
            "  1. 1_analise_exploratoria.ipynb\n",
            "  2. 2_preprocess.ipynb\n",
            "  3. v3_model.ipynb\n",
            "\n",
            "Etapas deste notebook:\n",
            "  - Carregar dados e fazer feature engineering\n",
            "  - Definir espaço de busca de hiperparâmetros\n",
            "  - Treinar com TimeSeriesSplit e múltiplos seeds\n",
            "  - Selecionar melhor configuração por Competition Score\n",
            "  - Treinar modelo final e gerar submission\n",
            "\n",
            "NOTA: Este notebook implementa busca de parâmetros otimizada\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TRAINING V3 - MLP COM OTIMIZAÇÃO DE HIPERPARÂMETROS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nNotebooks Reutilizados:\")\n",
        "print(\"  1. 1_analise_exploratoria.ipynb\")\n",
        "print(\"  2. 2_preprocess.ipynb\")\n",
        "print(\"  3. v3_model.ipynb\")\n",
        "\n",
        "print(\"\\nEtapas deste notebook:\")\n",
        "print(\"  - Carregar dados e fazer feature engineering\")\n",
        "print(\"  - Definir espaço de busca de hiperparâmetros\")\n",
        "print(\"  - Treinar com TimeSeriesSplit e múltiplos seeds\")\n",
        "print(\"  - Selecionar melhor configuração por Competition Score\")\n",
        "print(\"  - Treinar modelo final e gerar submission\")\n",
        "\n",
        "print(\"\\nNOTA: Este notebook implementa busca de parâmetros otimizada\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Importações carregadas\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.base import clone\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "DATA_DIR = Path('.')\n",
        "TRAIN_DIR = DATA_DIR / 'train'\n",
        "TEST_PATH = DATA_DIR / 'test.csv'\n",
        "\n",
        "print(\"Importações carregadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Safe MSE e métricas configuradas\n"
          ]
        }
      ],
      "source": [
        "## Safe MSE com clipping para evitar overflow\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "np.seterr(over='ignore', invalid='ignore')\n",
        "\n",
        "_ORIG_MSE = metrics.mean_squared_error\n",
        "PRED_MAX = 1e10\n",
        "PRED_MIN = 0.0\n",
        "LOG_MAX = 20.0\n",
        "LOG_MIN = -20.0\n",
        "\n",
        "def _safe_mse(y_true, y_pred, sample_weight=None, multioutput=\"uniform_average\"):\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    y_true = np.nan_to_num(y_true, nan=PRED_MIN, posinf=PRED_MAX, neginf=PRED_MIN)\n",
        "    y_pred = np.nan_to_num(y_pred, nan=PRED_MIN, posinf=PRED_MAX, neginf=PRED_MIN)\n",
        "    y_true = np.clip(y_true, PRED_MIN, PRED_MAX)\n",
        "    y_pred = np.clip(y_pred, PRED_MIN, PRED_MAX)\n",
        "    return _ORIG_MSE(y_true, y_pred, sample_weight=sample_weight, multioutput=multioutput)\n",
        "\n",
        "metrics.mean_squared_error = _safe_mse\n",
        "\n",
        "def competition_score(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    den = np.where(np.abs(y_true) < 1e-9, 1.0, np.abs(y_true))\n",
        "    ape = np.abs(y_true - y_pred) / den\n",
        "    frac_le1 = float(np.mean(ape <= 1.0))\n",
        "    if 1.0 - frac_le1 > 0.30:\n",
        "        return {\"score\": 0.0, \"frac_le1\": frac_le1}\n",
        "    mask = ape <= 1.0\n",
        "    if not np.any(mask):\n",
        "        return {\"score\": 0.0, \"frac_le1\": 0.0}\n",
        "    mape_subset = float(np.mean(ape[mask]))\n",
        "    scaled_mape = float(mape_subset / max(frac_le1, 1e-12))\n",
        "    return {\"score\": float(1.0 - scaled_mape), \"frac_le1\": frac_le1}\n",
        "\n",
        "print(\"Safe MSE e métricas configuradas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape original: (5433, 11)\n",
            "Após parsing: 5433/5433 linhas válidas\n",
            "Período: 2019-01-01 a 2024-07-01\n",
            "Setores: 95\n"
          ]
        }
      ],
      "source": [
        "## Carregar e Processar Dados\n",
        "\n",
        "new_house = pd.read_csv(TRAIN_DIR / 'new_house_transactions.csv')\n",
        "print(f\"Shape original: {new_house.shape}\")\n",
        "\n",
        "m = (\n",
        "    new_house['month']\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    .str.replace(r'[\\u2013\\u2014]', '-', regex=True)\n",
        "    .str.replace('.', '', regex=False)\n",
        "    .str.replace('Sept', 'Sep', regex=False)\n",
        ")\n",
        "\n",
        "d = pd.to_datetime(m, format='%Y-%b', errors='coerce')\n",
        "d = d.fillna(pd.to_datetime(m, format='%Y-%m', errors='coerce'))\n",
        "d = d.fillna(pd.to_datetime(m, format='%b %Y', errors='coerce'))\n",
        "d = d.fillna(pd.to_datetime(m, format='%Y %b', errors='coerce'))\n",
        "d = d.fillna(pd.to_datetime(m, errors='coerce'))\n",
        "\n",
        "new_house['date'] = d\n",
        "new_house['sector_id'] = (\n",
        "    new_house['sector']\n",
        "    .astype(str)\n",
        "    .str.extract(r'(\\d+)', expand=False)\n",
        "    .astype('int64')\n",
        ")\n",
        "\n",
        "before = new_house.shape[0]\n",
        "new_house = new_house.dropna(subset=['date'])\n",
        "after = new_house.shape[0]\n",
        "\n",
        "print(f\"Após parsing: {after}/{before} linhas válidas\")\n",
        "print(f\"Período: {new_house['date'].min().date()} a {new_house['date'].max().date()}\")\n",
        "print(f\"Setores: {new_house['sector_id'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape após feature eng: (5433, 30)\n",
            "Features: 27\n"
          ]
        }
      ],
      "source": [
        "## Feature Engineering\n",
        "\n",
        "df = new_house.copy()\n",
        "df = df.sort_values(['sector_id', 'date']).reset_index(drop=True)\n",
        "\n",
        "df['y'] = df['amount_new_house_transactions']\n",
        "df['area'] = df['area_new_house_transactions']\n",
        "df['price'] = df['price_new_house_transactions']\n",
        "df['num'] = df['num_new_house_transactions']\n",
        "\n",
        "df['price_x_area'] = df['price'] * df['area']\n",
        "df['price_per_unit'] = df['price'] / (df['num'] + 1e-6)\n",
        "df['area_per_unit'] = df['area'] / (df['num'] + 1e-6)\n",
        "\n",
        "for lag in [1, 3, 6]:\n",
        "    df[f'price_lag{lag}'] = df.groupby('sector_id')['price'].shift(lag)\n",
        "    df[f'area_lag{lag}'] = df.groupby('sector_id')['area'].shift(lag)\n",
        "    df[f'num_lag{lag}'] = df.groupby('sector_id')['num'].shift(lag)\n",
        "    df[f'amount_lag{lag}'] = df.groupby('sector_id')['y'].shift(lag)\n",
        "\n",
        "for window in [3, 6]:\n",
        "    df[f'price_ma{window}'] = df.groupby('sector_id')['price'].transform(\n",
        "        lambda x: x.rolling(window, min_periods=1).mean()\n",
        "    )\n",
        "    df[f'area_ma{window}'] = df.groupby('sector_id')['area'].transform(\n",
        "        lambda x: x.rolling(window, min_periods=1).mean()\n",
        "    )\n",
        "    df[f'amount_ma{window}'] = df.groupby('sector_id')['y'].transform(\n",
        "        lambda x: x.rolling(window, min_periods=1).mean()\n",
        "    )\n",
        "\n",
        "df['month'] = df['date'].dt.month\n",
        "df['quarter'] = df['date'].dt.quarter\n",
        "df['year'] = df['date'].dt.year\n",
        "\n",
        "lag_cols = [c for c in df.columns if 'lag' in c or 'ma' in c]\n",
        "for col in lag_cols:\n",
        "    df[col] = df.groupby('sector_id')[col].ffill().bfill()\n",
        "\n",
        "feature_cols = [\n",
        "    'area', 'price', 'num', 'price_x_area', 'price_per_unit', 'area_per_unit',\n",
        "    'price_lag1', 'price_lag3', 'price_lag6',\n",
        "    'area_lag1', 'area_lag3', 'area_lag6',\n",
        "    'num_lag1', 'num_lag3', 'num_lag6',\n",
        "    'amount_lag1', 'amount_lag3', 'amount_lag6',\n",
        "    'price_ma3', 'price_ma6', 'area_ma3', 'area_ma6', 'amount_ma3', 'amount_ma6',\n",
        "    'month', 'quarter', 'year'\n",
        "]\n",
        "\n",
        "for col in feature_cols:\n",
        "    if df[col].isna().sum() > 0:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "df_clean = df[['sector_id', 'date', 'y'] + feature_cols].dropna(subset=['sector_id', 'date', 'y']).reset_index(drop=True)\n",
        "\n",
        "print(f\"Shape após feature eng: {df_clean.shape}\")\n",
        "print(f\"Features: {len(feature_cols)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buscando hiperparâmetros...\n",
            "\n",
            "Comb 1: Score=0.7079 MSE=132779519718414\n",
            "Comb 2: Score=0.7820 MSE=399016297\n",
            "Comb 3: Score=0.6844 MSE=103585631184632\n"
          ]
        }
      ],
      "source": [
        "## Busca de Hiperparâmetros com TimeSeriesSplit\n",
        "\n",
        "X = df_clean[feature_cols].copy()\n",
        "y = df_clean['y'].values\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    ('scaler', StandardScaler(), feature_cols)\n",
        "], remainder='drop')\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(128, 64, 32), (256, 128, 64), (256, 128)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'alpha': [1e-5, 1e-4, 1e-3],\n",
        "    'batch_size': [128, 256],\n",
        "    'learning_rate_init': [0.001, 0.01]\n",
        "}\n",
        "\n",
        "param_sampler = ParameterSampler(param_grid, n_iter=12, random_state=42)\n",
        "seeds = [42, 123, 456]\n",
        "\n",
        "results = []\n",
        "print(\"Buscando hiperparâmetros...\\n\")\n",
        "\n",
        "for iter_idx, params in enumerate(param_sampler, start=1):\n",
        "    best_for_params = None\n",
        "    \n",
        "    for seed in seeds:\n",
        "        model = MLPRegressor(\n",
        "            random_state=seed,\n",
        "            early_stopping=True,\n",
        "            max_iter=400,\n",
        "            n_iter_no_change=30,\n",
        "            validation_fraction=0.15,\n",
        "            **params,\n",
        "            solver=\"adam\",\n",
        "        )\n",
        "        \n",
        "        oof_pred = np.zeros_like(y, dtype=float)\n",
        "        fold_scores = []\n",
        "        \n",
        "        for fold, (tr_idx, te_idx) in enumerate(tscv.split(X)):\n",
        "            try:\n",
        "                Xtr, Xte = X.iloc[tr_idx], X.iloc[te_idx]\n",
        "                ytr_log = y_log[tr_idx]\n",
        "                \n",
        "                pipe = Pipeline([('prep', clone(preprocess)), ('mlp', clone(model))])\n",
        "                pipe.fit(Xtr, ytr_log)\n",
        "                \n",
        "                yhat_log = pipe.predict(Xte)\n",
        "                yhat_log = np.clip(yhat_log, LOG_MIN, LOG_MAX)\n",
        "                yhat = np.expm1(yhat_log)\n",
        "                yhat = np.clip(yhat, PRED_MIN, PRED_MAX)\n",
        "                \n",
        "                oof_pred[te_idx] = yhat\n",
        "                fold_scores.append(mean_squared_error(y[te_idx], yhat))\n",
        "                \n",
        "            except Exception as e:\n",
        "                fold_scores = None\n",
        "                break\n",
        "        \n",
        "        if fold_scores is not None:\n",
        "            oof_pred = np.clip(oof_pred, PRED_MIN, PRED_MAX)\n",
        "            comp_score = competition_score(y, oof_pred)\n",
        "            mse = mean_squared_error(y, oof_pred)\n",
        "            \n",
        "            result = {\n",
        "                'params': {**params, 'random_state': seed},\n",
        "                'mse': mse,\n",
        "                'comp_score': comp_score['score']\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            if best_for_params is None or comp_score['score'] > best_for_params['comp_score']:\n",
        "                best_for_params = result\n",
        "    \n",
        "    if best_for_params:\n",
        "        print(f\"Comb {iter_idx}: Score={best_for_params['comp_score']:.4f} MSE={best_for_params['mse']:.0f}\")\n",
        "\n",
        "best_result = sorted(results, key=lambda d: (-d['comp_score'], d['mse']))[0]\n",
        "print(f\"\\nMelhor resultado:\")\n",
        "print(f\"  Score: {best_result['comp_score']:.4f}\")\n",
        "print(f\"  MSE: {best_result['mse']:.0f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Treinar Modelo Final\n",
        "\n",
        "print(\"Treinando modelo final...\")\n",
        "\n",
        "final_model = MLPRegressor(\n",
        "    **{k: v for k, v in best_result['params'].items() if k != 'random_state'},\n",
        "    random_state=best_result['params']['random_state'],\n",
        "    early_stopping=True,\n",
        "    max_iter=400,\n",
        "    n_iter_no_change=30,\n",
        "    validation_fraction=0.15,\n",
        "    solver=\"adam\"\n",
        ")\n",
        "\n",
        "final_pipe = Pipeline([('prep', preprocess), ('mlp', final_model)])\n",
        "final_pipe.fit(X, y_log)\n",
        "\n",
        "import joblib\n",
        "joblib.dump(final_pipe, 'mlp_model_v3.joblib')\n",
        "joblib.dump(feature_cols, 'feature_cols_v3.joblib')\n",
        "\n",
        "print(\"Modelo final treinado e salvo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Gerar Submission\n",
        "\n",
        "test_df = pd.read_csv(TEST_PATH)\n",
        "\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "\n",
        "if 'id' in test_df.columns:\n",
        "    test_ids = test_df['id'].values\n",
        "    \n",
        "    if set(feature_cols).issubset(set(test_df.columns)):\n",
        "        X_test = test_df[feature_cols].copy()\n",
        "        \n",
        "        for col in feature_cols:\n",
        "            if X_test[col].isna().sum() > 0:\n",
        "                X_test[col] = X_test[col].fillna(X[col].median())\n",
        "        \n",
        "        y_test_log = final_pipe.predict(X_test)\n",
        "        y_test_log = np.clip(y_test_log, LOG_MIN, LOG_MAX)\n",
        "        y_test = np.expm1(y_test_log)\n",
        "        y_test = np.clip(y_test, PRED_MIN, PRED_MAX)\n",
        "        \n",
        "        submission = pd.DataFrame({\n",
        "            'id': test_ids,\n",
        "            'amount_new_house_transactions': y_test\n",
        "        })\n",
        "        \n",
        "        submission.to_csv('submission_v3.csv', index=False)\n",
        "        print(f\"Submission salva em 'submission_v3.csv'\")\n",
        "        print(f\"  Shape: {submission.shape}\")\n",
        "        print(f\"  Min: {submission['amount_new_house_transactions'].min():,.1f}\")\n",
        "        print(f\"  Max: {submission['amount_new_house_transactions'].max():,.1f}\")\n",
        "        print(f\"  Mean: {submission['amount_new_house_transactions'].mean():,.1f}\")\n",
        "    else:\n",
        "        print(\"Features não encontradas em test.csv\")\n",
        "else:\n",
        "    print(\"Coluna 'id' não encontrada em test.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Resumo Final\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RESUMO FINAL - TRAINING V3\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nDados: {df_clean.shape[0]} linhas, {len(feature_cols)} features\")\n",
        "print(f\"Período: {df_clean['date'].min().date()} a {df_clean['date'].max().date()}\")\n",
        "print(f\"Setores: {df_clean['sector_id'].nunique()}\")\n",
        "\n",
        "print(f\"\\nMelhor Configuração:\")\n",
        "print(f\"  Hidden layers: {best_result['params']['hidden_layer_sizes']}\")\n",
        "print(f\"  Activation: {best_result['params']['activation']}\")\n",
        "print(f\"  Alpha: {best_result['params']['alpha']}\")\n",
        "print(f\"  Batch size: {best_result['params']['batch_size']}\")\n",
        "print(f\"  Learning rate: {best_result['params']['learning_rate_init']}\")\n",
        "print(f\"  Seed: {best_result['params']['random_state']}\")\n",
        "\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Competition Score: {best_result['comp_score']:.4f}\")\n",
        "print(f\"  MSE: {best_result['mse']:.0f}\")\n",
        "\n",
        "print(f\"\\nArquivos gerados:\")\n",
        "print(f\"  mlp_model_v3.joblib\")\n",
        "print(f\"  feature_cols_v3.joblib\")\n",
        "print(f\"  submission_v3.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Salvar Métricas para Comparação\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "metrics_v3 = {\n",
        "    \"versao\": \"v3\",\n",
        "    \"data\": datetime.now().isoformat(),\n",
        "    \"modelo\": \"MLPRegressor com Busca de Hiperparâmetros\",\n",
        "    \"validacao\": \"TimeSeriesSplit + Múltiplos Seeds\",\n",
        "    \"features\": len(feature_cols),\n",
        "    \"observacoes\": df_clean.shape[0],\n",
        "    \"setores\": df_clean['sector_id'].nunique(),\n",
        "    \"metricas_melhor_resultado\": {\n",
        "        \"rmse\": float(best_result['mse'] ** 0.5),\n",
        "        \"mse\": float(best_result['mse']),\n",
        "        \"competition_score\": float(best_result['comp_score'])\n",
        "    },\n",
        "    \"melhor_configuracao\": {\n",
        "        \"hidden_layers\": best_result['params']['hidden_layer_sizes'],\n",
        "        \"activation\": best_result['params']['activation'],\n",
        "        \"alpha\": best_result['params']['alpha'],\n",
        "        \"batch_size\": best_result['params']['batch_size'],\n",
        "        \"learning_rate\": best_result['params']['learning_rate_init'],\n",
        "        \"seed\": int(best_result['params']['random_state'])\n",
        "    },\n",
        "    \"resumo_busca\": {\n",
        "        \"total_combinacoes_testadas\": len(results),\n",
        "        \"parametros_grid\": {\n",
        "            \"hidden_layer_sizes\": 3,\n",
        "            \"activation\": 2,\n",
        "            \"alpha\": 3,\n",
        "            \"batch_size\": 2,\n",
        "            \"learning_rate_init\": 2\n",
        "        },\n",
        "        \"seeds_utilizados\": 3\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('metricas_v3.json', 'w') as f:\n",
        "    json.dump(metrics_v3, f, indent=2)\n",
        "\n",
        "print(\"\\nMétricas salvas em 'metricas_v3.json'\")\n",
        "print(\"\\nResumo Final v3 (Melhor Resultado):\")\n",
        "print(f\"  RMSE: {metrics_v3['metricas_melhor_resultado']['rmse']:.2f}\")\n",
        "print(f\"  MSE: {metrics_v3['metricas_melhor_resultado']['mse']:.2f}\")\n",
        "print(f\"  Competition Score: {metrics_v3['metricas_melhor_resultado']['competition_score']:.4f}\")\n",
        "print(f\"\\nTotal de combinações testadas: {metrics_v3['resumo_busca']['total_combinacoes_testadas']}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
